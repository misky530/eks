内容太长，我直接给你一个完整的合并版本。你可以复制保存：

---

由于内容非常长（约2000行），我分段给你。先给你第一部分：

## 完整运维知识库 - 第1部分（总览 + EKS）

```markdown
# IoT Platform 运维知识库

> 基于 Kubernetes 的 IoT 数据处理平台 - 运维文档
> 创建日期：2025-11-27

---

# 目录

1. [项目概述](#项目概述)
2. [EKS Troubleshooting](#eks-troubleshooting)
3. [ArgoCD 指南](#argocd-指南)
4. [Kafka (Strimzi) 指南](#kafka-strimzi-指南)
5. [Prometheus Stack 指南](#prometheus-stack-指南)

---

# 项目概述

## 技术栈

| 组件 | 技术选型 | 用途 |
|------|----------|------|
| Kubernetes | AWS EKS | 容器编排 |
| GitOps | ArgoCD | 持续部署 |
| 消息队列 | Kafka (Strimzi) | 数据缓冲和分发 |
| 监控 | Prometheus + Grafana | 指标采集和可视化 |
| IaC | eksctl | 基础设施即代码 |

## 架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                         AWS EKS Cluster                         │
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐ │
│  │   ArgoCD    │  │   Kafka     │  │   Prometheus + Grafana  │ │
│  │   (GitOps)  │  │  (Strimzi)  │  │     (Monitoring)        │ │
│  └─────────────┘  └─────────────┘  └─────────────────────────┘ │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    待部署组件                            │   │
│  │  MQTT Broker  →  Kafka Bridge  →  Tenant Processors     │   │
│  └─────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────┘
```

## 当前部署状态

```
✅ EKS集群 (iot-platform, us-east-1)
✅ ArgoCD (GitOps持续部署)
✅ Kafka (Strimzi Operator + iot-cluster)
✅ Prometheus + Grafana (监控可视化)

⏳ 待部署：
   - MQTT Broker (EMQX)
   - Kafka Bridge
   - Tenant Processor
```

## 访问入口

| 服务 | 类型 | 获取地址命令 |
|------|------|-------------|
| ArgoCD | LoadBalancer | `kubectl get svc argocd-server -n argocd` |
| Grafana | LoadBalancer | `kubectl get svc prometheus-stack-grafana -n monitoring` |
| Kafka | ClusterIP | `iot-cluster-kafka-bootstrap.kafka:9092` |

## 项目结构

```
iot-platform/
├── eksctl/
│   ├── cluster.yaml
│   └── nodegroup.yaml
├── argocd-apps/
│   └── platform/
│       ├── strimzi-operator.yaml
│       └── prometheus-stack.yaml
├── helm-charts/
│   └── kafka-cluster/
│       └── kafka.yaml
└── docs/
    └── ops-handbook.md    # 本文件
```

---

# EKS Troubleshooting

## 环境信息

| 项目 | 值 |
|------|-----|
| EKS版本 | 1.29 |
| 区域 | us-east-1 |
| 工具版本 | eksctl 0.218.0 |
| 账号类型 | Free Tier ($300额度) |

## 问题速查表

| 错误关键词 | 可能原因 | 快速解决 |
|------------|----------|----------|
| `not eligible for Free Tier` | 实例类型不在Free Tier列表 | 改用t3.small/t3.micro |
| `TerminationProtection is enabled` | CloudFormation栈有删除保护 | 先禁用保护再删除 |
| `exceeded max wait time` | eksctl等待超时 | 检查实际状态，可能仍在创建中 |
| `dial tcp [::1]:8080` | kubectl未配置 | 运行aws eks update-kubeconfig |
| `No node group found` | 节点组创建失败被删除 | 查看CloudFormation事件找原因 |
| `VcpuLimitExceeded` | vCPU配额超限 | 优化资源或申请提升配额 |

## 问题1：Free Tier实例类型限制

### 现象

```
InvalidParameterCombination - The specified instance type is not eligible for Free Tier
```

### 解决方案

查询可用的Free Tier实例类型：

```bash
aws ec2 describe-instance-types \
  --filters "Name=free-tier-eligible,Values=true" \
  --query 'InstanceTypes[*].[InstanceType,VCpuInfo.DefaultVCpus,MemoryInfo.SizeInMiB]' \
  --output table
```

常见Free Tier实例类型：

| 实例类型 | vCPU | 内存 |
|----------|------|------|
| t3.micro | 2 | 1GB |
| t3.small | 2 | 2GB |
| t4g.micro | 2 | 1GB |
| t4g.small | 2 | 2GB |

## 问题2：vCPU配额限制

### 现象

```
VcpuLimitExceeded - You have requested more vCPU capacity than your current vCPU limit of 8 allows
```

### 诊断

```bash
# 查看ASG扩容失败原因
aws autoscaling describe-scaling-activities \
  --auto-scaling-group-name <asg-name> \
  --query 'Activities[*].[StatusCode,StatusMessage]' \
  --output table
```

### 解决方案

方案A：申请提升配额
```bash
aws service-quotas request-service-quota-increase \
  --service-code ec2 \
  --quota-code L-1216C47A \
  --desired-value 16
```

方案B：优化现有资源（禁用非必要组件）

## 问题3：CloudFormation栈删除保护

### 现象

```
Stack [xxx] cannot be deleted while TerminationProtection is enabled
```

### 解决方案

```bash
# 禁用终止保护
aws cloudformation update-termination-protection \
  --no-enable-termination-protection \
  --stack-name <stack-name>

# 删除栈
aws cloudformation delete-stack --stack-name <stack-name>

# 等待删除完成
aws cloudformation wait stack-delete-complete --stack-name <stack-name>
```

## 问题4：kubectl连接失败

### 现象

```
dial tcp [::1]:8080: connectex: No connection could be made
```

### 解决方案

```bash
aws eks update-kubeconfig --region us-east-1 --name iot-platform
kubectl get nodes
```

## AWS账号限制查询

```bash
# 查询Free Tier实例类型
aws ec2 describe-instance-types \
  --filters "Name=free-tier-eligible,Values=true" \
  --query 'InstanceTypes[*].InstanceType' \
  --output text

# 查询vCPU配额
aws service-quotas get-service-quota \
  --service-code ec2 \
  --quota-code L-1216C47A \
  --query 'Quota.Value'
```

### 新账号常见限制

| 限制类型 | 典型默认值 |
|----------|------------|
| 按需实例vCPU | 8个 |
| Spot实例vCPU | 5个 |
| 弹性IP | 5个/region |

## 参考配置

### cluster.yaml

```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: iot-platform
  region: us-east-1
  version: "1.29"

iam:
  withOIDC: true

vpc:
  cidr: 10.0.0.0/16
  nat:
    gateway: Single
  clusterEndpoints:
    publicAccess: true
    privateAccess: true

addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
  - name: kube-proxy
    version: latest
  - name: aws-ebs-csi-driver
    version: latest
    attachPolicyARNs:
      - arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy
```

### nodegroup.yaml（Free Tier兼容）

```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: iot-platform
  region: us-east-1

managedNodeGroups:
  - name: workers
    instanceType: t3.small
    desiredCapacity: 2
    minSize: 1
    maxSize: 3
    volumeSize: 30
    volumeType: gp3
    spot: false
    
    labels:
      role: worker
      environment: dev
    
    iam:
      withAddonPolicies:
        ebs: true
        albIngress: true
        cloudWatch: true
```
```

---

需要我继续给你第2部分（ArgoCD + Kafka + Prometheus）吗？

## 完整运维知识库 - 第2部分（ArgoCD + Kafka + Prometheus）

```markdown
---

# ArgoCD 指南

## 概述

ArgoCD是声明式GitOps持续部署工具：
- 监控Git仓库中的Kubernetes配置
- 自动将集群状态同步到Git定义的期望状态
- 提供可视化UI管理应用部署

**核心理念**：Git是唯一真相来源

## 安装步骤

```bash
# 1. 创建Namespace
kubectl create namespace argocd

# 2. 安装ArgoCD
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

# 3. 等待Pod就绪
kubectl wait --for=condition=Ready pods --all -n argocd --timeout=300s

# 4. 配置公网访问
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'

# 5. 获取地址
kubectl get svc argocd-server -n argocd

# 6. 获取密码
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
echo
```

**登录信息**：用户名 `admin`，密码为上面命令输出

## 安装的组件

| 组件 | 作用 |
|------|------|
| argocd-server | API服务器和Web UI |
| argocd-repo-server | Git仓库管理 |
| argocd-application-controller | 应用同步控制器 |
| argocd-dex-server | SSO认证 |
| argocd-redis | 缓存 |

## Application配置示例

### 使用Helm Chart

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: my-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://charts.example.com
    chart: my-chart
    targetRevision: 1.0.0
    helm:
      values: |
        key: value
  destination:
    server: https://kubernetes.default.svc
    namespace: my-namespace
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

## 同步策略说明

| 配置 | 作用 |
|------|------|
| `automated.prune` | 删除Git中已移除的资源 |
| `automated.selfHeal` | 自动修复集群漂移 |
| `CreateNamespace=true` | 自动创建namespace |
| `ServerSideApply=true` | 服务端应用（大资源推荐） |

## 常用操作

```bash
# 查看应用
kubectl get applications -n argocd

# 查看应用详情
kubectl describe application <app-name> -n argocd

# 删除应用
kubectl delete application <app-name> -n argocd
```

## 故障排查

### 应用同步失败

```bash
kubectl describe application <app-name> -n argocd
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller
```

### UI无法访问

```bash
kubectl get pods -n argocd
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server
```

---

# Kafka (Strimzi) 指南

## 概述

Strimzi是CNCF项目，通过Operator模式在Kubernetes上管理Kafka：
- 声明式配置（CRD）
- 自动化运维（滚动更新、扩缩容）
- 证书管理

## 组件说明

| 组件 | 作用 |
|------|------|
| Strimzi Cluster Operator | 管理Kafka集群 |
| Kafka Broker | 消息代理 |
| ZooKeeper | 集群协调 |
| Entity Operator | 管理Topic和User |

## 部署步骤

### 步骤1：部署Strimzi Operator

创建 `argocd-apps/platform/strimzi-operator.yaml`：

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: strimzi-operator
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://strimzi.io/charts/
    chart: strimzi-kafka-operator
    targetRevision: 0.44.0
    helm:
      values: |
        resources:
          limits:
            memory: 384Mi
            cpu: 500m
          requests:
            memory: 256Mi
            cpu: 100m
  destination:
    server: https://kubernetes.default.svc
    namespace: kafka
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

```bash
kubectl apply -f argocd-apps/platform/strimzi-operator.yaml
kubectl wait --for=condition=Ready pods -l name=strimzi-cluster-operator -n kafka --timeout=300s
```

### 步骤2：部署Kafka集群

创建 `helm-charts/kafka-cluster/kafka.yaml`：

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: iot-cluster
  namespace: kafka
spec:
  kafka:
    version: 3.8.0
    replicas: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
      default.replication.factor: 1
      min.insync.replicas: 1
    storage:
      type: ephemeral
    resources:
      requests:
        memory: 256Mi
        cpu: 100m
      limits:
        memory: 512Mi
        cpu: 300m
  zookeeper:
    replicas: 1
    storage:
      type: ephemeral
    resources:
      requests:
        memory: 128Mi
        cpu: 50m
      limits:
        memory: 256Mi
        cpu: 200m
  entityOperator:
    topicOperator:
      resources:
        requests:
          memory: 64Mi
          cpu: 25m
        limits:
          memory: 128Mi
          cpu: 100m
    userOperator:
      resources:
        requests:
          memory: 64Mi
          cpu: 25m
        limits:
          memory: 128Mi
          cpu: 100m
```

```bash
kubectl apply -f helm-charts/kafka-cluster/kafka.yaml
kubectl wait kafka/iot-cluster --for=condition=Ready --timeout=300s -n kafka
```

## 学习环境 vs 生产环境

| 配置项 | 学习环境 | 生产环境 |
|--------|----------|----------|
| Kafka副本 | 1 | 3+ |
| ZooKeeper副本 | 1 | 3 |
| 存储类型 | ephemeral | persistent-claim |
| replication.factor | 1 | 3 |

## 常用操作

```bash
# 查看Kafka状态
kubectl get kafka -n kafka

# 查看所有Pod
kubectl get pods -n kafka

# 测试消息收发
kubectl exec -it iot-cluster-kafka-0 -n kafka -- bash
# 生产消息
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
# 消费消息
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
```

## Kafka连接信息

| 用途 | 地址 |
|------|------|
| 集群内部 | `iot-cluster-kafka-bootstrap.kafka:9092` |
| 完整地址 | `iot-cluster-kafka-bootstrap.kafka.svc.cluster.local:9092` |

---

# Prometheus Stack 指南

## 概述

kube-prometheus-stack 包含：

| 组件 | 作用 | 资源消耗 |
|------|------|----------|
| Prometheus | 指标采集存储 | 高 |
| Grafana | 可视化仪表盘 | 中 |
| AlertManager | 告警管理 | 低 |
| Prometheus Operator | 管理实例 | 低 |
| kube-state-metrics | 集群状态指标 | 低 |
| node-exporter | 节点指标 | 低×节点数 |

## 问题与解决方案

### 问题1：Pod数量超过节点限制

**现象**：
```
Warning  FailedScheduling  0/2 nodes are available: 2 Too many pods.
```

**原因**：AWS EKS节点Pod数量限制

| 实例类型 | 最大Pod数 |
|----------|-----------|
| t3.micro | 4 |
| t3.small | 11 |
| t3.medium | 17 |
| t3.large | 35 |

**解决方案**：禁用非必要组件
- `alertmanager.enabled: false`
- `nodeExporter.enabled: false`

### 问题2：Namespace卡在Terminating

**现象**：
```bash
kubectl get namespaces
# monitoring   Terminating   179m
```

**原因**：资源带有finalizers阻止删除

**解决方案**：
```bash
# 清除各类资源的finalizers
kubectl get jobs -n monitoring -o name | xargs -I {} kubectl patch {} -n monitoring -p '{"metadata":{"finalizers":[]}}' --type=merge

kubectl get serviceaccounts -n monitoring -o name | xargs -I {} kubectl patch {} -n monitoring -p '{"metadata":{"finalizers":[]}}' --type=merge

kubectl get rolebindings -n monitoring -o name | xargs -I {} kubectl patch {} -n monitoring -p '{"metadata":{"finalizers":[]}}' --type=merge

kubectl get roles -n monitoring -o name | xargs -I {} kubectl patch {} -n monitoring -p '{"metadata":{"finalizers":[]}}' --type=merge

# 如果还卡住，清除所有资源
kubectl get all -n monitoring -o name | xargs -I {} kubectl patch {} -n monitoring -p '{"metadata":{"finalizers":[]}}' --type=merge
```

### 问题3：Operator因TLS Secret缺失无法启动

**现象**：
```
MountVolume.SetUp failed for volume "tls-secret" : secret "prometheus-stack-kube-prom-admission" not found
```

**解决方案**：完全禁用webhooks
```yaml
prometheusOperator:
  admissionWebhooks:
    enabled: false
    patch:
      enabled: false
  tls:
    enabled: false
```

## 学习环境完整配置

创建 `argocd-apps/platform/prometheus-stack.yaml`：

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: prometheus-stack
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    chart: kube-prometheus-stack
    targetRevision: 65.1.0
    helm:
      values: |
        prometheus:
          prometheusSpec:
            resources:
              requests:
                memory: 128Mi
                cpu: 50m
              limits:
                memory: 256Mi
                cpu: 200m
            retention: 1d
        
        grafana:
          adminPassword: admin123
          resources:
            requests:
              memory: 64Mi
              cpu: 25m
            limits:
              memory: 128Mi
              cpu: 100m
          service:
            type: LoadBalancer
        
        alertmanager:
          enabled: false
        
        nodeExporter:
          enabled: false
        
        kubeEtcd:
          enabled: false
        kubeControllerManager:
          enabled: false
        kubeScheduler:
          enabled: false
        kubeProxy:
          enabled: false
        
        prometheusOperator:
          admissionWebhooks:
            enabled: false
            patch:
              enabled: false
          tls:
            enabled: false
          resources:
            requests:
              memory: 64Mi
              cpu: 25m
            limits:
              memory: 128Mi
              cpu: 100m
        
        kubeStateMetrics:
          enabled: true
        
        kube-state-metrics:
          resources:
            requests:
              memory: 32Mi
              cpu: 10m
            limits:
              memory: 64Mi
              cpu: 50m
  destination:
    server: https://kubernetes.default.svc
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
```

## 部署和验证

```bash
kubectl apply -f argocd-apps/platform/prometheus-stack.yaml

# 查看状态
kubectl get pods -n monitoring

# 获取Grafana地址
kubectl get svc -n monitoring prometheus-stack-grafana
```

**Grafana登录**：admin / admin123

## 最佳实践

### 删除监控栈的正确顺序

1. 先删除ArgoCD Application
2. 等待资源清理
3. 清理残留finalizers
4. 删除namespace

```bash
kubectl delete application prometheus-stack -n argocd
sleep 30
kubectl delete namespace monitoring --force --grace-period=0
# 如果卡住，执行finalizer清理命令
```

### Pod数量估算（学习环境）

| 组件 | Pod数 | 可禁用 |
|------|-------|--------|
| Prometheus | 1 | 否 |
| Grafana | 1 | 否 |
| Operator | 1 | 否 |
| kube-state-metrics | 1 | 可选 |
| AlertManager | 1 | 是 |
| node-exporter | N | 是 |

**学习环境最小Pod数：4个**

---

# 经验教训总结

## AWS Free Tier 关键限制

1. **实例类型**：只能用t3.small, t3.micro等
2. **vCPU配额**：默认8个，2节点t3.small刚好用完
3. **Pod数量**：t3.small每节点最多11个

## 资源优化策略

1. 禁用非必要组件（alertmanager, node-exporter）
2. 使用最小资源请求
3. 合理规划Pod分布
4. 先部署核心组件，验证后再扩展

## 常用诊断命令速查

```bash
# 查看节点Pod分布
kubectl get pods -A -o wide --no-headers | awk '{print $8}' | sort | uniq -c

# 查看Pod调度失败原因
kubectl describe pod <pod-name> -n <namespace> | tail -20

# 查看ASG扩容失败原因
aws autoscaling describe-scaling-activities \
  --auto-scaling-group-name <asg-name> \
  --query 'Activities[*].[StatusCode,StatusMessage]' \
  --output table

# 强制清理namespace
kubectl get all -n <namespace> -o name | xargs -I {} kubectl patch {} -n <namespace> -p '{"metadata":{"finalizers":[]}}' --type=merge
```

---

# 更新日志

| 日期 | 更新内容 |
|------|----------|
| 2025-11-27 | 初始版本：EKS + ArgoCD + Kafka + Prometheus |
```

---

这是完整的运维知识库，你可以复制保存为 `ops-handbook.md`。

**主要内容**：
- 项目概述和架构
- EKS问题排查（Free Tier、vCPU配额、节点组）
- ArgoCD安装和使用
- Kafka (Strimzi) 部署
- Prometheus Stack部署及问题解决
- 经验教训总结

准备好后继续部署MQTT Broker吗？
